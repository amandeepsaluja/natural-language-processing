{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building Machine Learning Classifiers\n",
    "What is ML?\n",
    " - The field of study that gives computers the ability to learn without being explicity programmed. (By Arthur Samuel, 1959)\n",
    " - A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measures by P, improves with experience E. (By Tom Mitchell, 1998)\n",
    " - Algorithms that can figure out how to perform important tasks by generalizing from examples. (By Univeristy of Washington, 2012)\n",
    " - Practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. (By NVIDA, 2016)\n",
    " \n",
    "Two __broad types__ of ML:\n",
    "\n",
    "1. **Supervised Learning**: Inferring a function from labeled training data to make predictions on unseen data.\n",
    " - Eg: Predict whether any given email is spam based on known information about the email. \n",
    " \n",
    " \n",
    "2. **Unsupervised Learning**: Deriving structure from the data where we don't know the effect of any of the variales.\n",
    " - Eg: Based on the content of an email, group similar emails together in distinct folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation and Evaluation Metrics\n",
    "\n",
    "### Cross-Validation\n",
    "**Holdout Test Set**: Sample of data not used in fitting a model for the purpose of evaluating the model's ability to generalize unseen data.\n",
    "\n",
    "**K-Fold Cross-Validation**: The dull data set is divided into k-subsets and the holdout method is repeated k times. Each time, one of the k-subsets is used as the test set and the other k-1 (k minus 1) subsets are put together to be used to train the model.\n",
    "\n",
    "### Evaluation Metrics\n",
    "1. Accuracy (# predicted correctly / total # of observations) \n",
    "2. Percision (# predicted as spam that are actually spam / total # predicted as spam)\n",
    "3. Recall (# predicted as spam that are actually spam / total # that are actually spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forest makes use of Ensemble method which is a technique that creates multiple models and then combines them to produce better results than any of the single models individually.\n",
    "\n",
    "Random Forest is an Ensemble learning method that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction.\n",
    "\n",
    "**Benefits:**\n",
    " - Can be used for classification or regression.\n",
    " - Easily handles outliers, missing values, etc.\n",
    " - Accepts various types of inputs (continous, ordinal, etc.).\n",
    " - Less likely to overfit.\n",
    " - Outputs feature importance.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128    4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62    3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28    7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135    4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# defining parameters\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "#fitting and vectorizing count_vect to our body_text\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "\n",
    "# building a datafram\n",
    "X_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier Attributes and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_make_estimator', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# importing RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# exploring the RandomForestClassifier\n",
    "print(dir(RandomForestClassifier)) # we will be using feature_importances_, fit, and predict functions\n",
    "print(RandomForestClassifier()) # we will be using max_depth and n_estimators parameters\n",
    "\n",
    "# Note: RF is built on relatively few full build decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier through Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97127469, 0.97755835, 0.96675651, 0.96495957, 0.96136568])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our RF classifier\n",
    "rf = RandomForestClassifier(n_jobs=-1) # n_jobs=-1 allows model to run faster bu building the individual decision trees in parallel\n",
    "\n",
    "# setting up KFold\n",
    "k_fold = KFold(n_splits=5) # spilitting dataset into 5 subsets\n",
    "\n",
    "# scoring our model\n",
    "cross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# ''' \n",
    "#     Parameters for the model:\n",
    "#     rf = model used\n",
    "#     X_features = input features\n",
    "#     data['label'] = label\n",
    "#     cv = how we are splitting the original dataset\n",
    "#     scoring = metric of scoring to use our model\n",
    "#     n_jobs = allows model to run faster bu building the individual decision trees in parallel \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Model on a holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128    4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62    3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28    7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135    4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# defining parameters\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "#fitting and vectorizing count_vect to our body_text\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "\n",
    "# building a datafram\n",
    "X_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier through Holdout Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting test and train dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate our RF classifier\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20,n_jobs=-1)\n",
    "\n",
    "# fitting the rf model\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0601724589565967, 'body_len'),\n",
       " (0.04331012686364079, 7350),\n",
       " (0.03529203415761097, 4796),\n",
       " (0.031698826882396225, 1803),\n",
       " (0.02141447446567646, 2031),\n",
       " (0.019594484496858064, 5724),\n",
       " (0.017852883055057472, 392),\n",
       " (0.01703567710369901, 3134),\n",
       " (0.01652325505991233, 6285),\n",
       " (0.016355554031669006, 7027)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick look at feature importance\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making prediction\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# looking at actual performance metrics\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.596\n",
      "Accuracy 0.94\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}\\nRecall: {}\\nAccuracy {}\".format(round(precision, 3),\n",
    "                                                         round(recall, 3),\n",
    "                                                         round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean? (in context of spam model)\n",
    " - Precision of 100% measns when the model identified something as spam, it was 100% right.\n",
    " - Recall of 59.6% means of all the spam that comes into email, 59.4% was properly palced in the spam folder.\n",
    " - Accuracy of 94% means of all the spam that comes into email, 94.3% of the emails were correctly identified as spam or ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest model with grid-search\n",
    "Grid Search exhaustively search all parameter combination in a given grid to determine the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128    4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62    3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28    7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135    4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# defining parameters\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "#fitting and vectorizing count_vect to our body_text\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "\n",
    "# building a datafram\n",
    "X_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our own Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting test and train dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for RF classifier\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth,n_jobs=-1) # instantiate our RF classifier\n",
    "    rf_model = rf.fit(X_train, y_train) # fitting the rf model\n",
    "    y_pred = rf_model.predict(X_test) #making prediction \n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary') #generating result matrix \n",
    "    print(\"No. of Estimators used: {} / Depth: {} ---- Precision: {} Recall: {} Accuracy {}\".format(n_est, \n",
    "                                                                                                         depth,\n",
    "                                                                                                         round(precision, 3),\n",
    "                                                                                                         round(recall, 3),\n",
    "                                                                                                         round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Estimators used: 10 / Depth: 10 ---- Precision: 1.0 Recall: 0.213 Accuracy 0.89\n",
      "No. of Estimators used: 10 / Depth: 20 ---- Precision: 1.0 Recall: 0.568 Accuracy 0.94\n",
      "No. of Estimators used: 10 / Depth: 30 ---- Precision: 1.0 Recall: 0.69 Accuracy 0.957\n",
      "No. of Estimators used: 10 / Depth: None ---- Precision: 1.0 Recall: 0.768 Accuracy 0.968\n",
      "No. of Estimators used: 50 / Depth: 10 ---- Precision: 1.0 Recall: 0.239 Accuracy 0.894\n",
      "No. of Estimators used: 50 / Depth: 20 ---- Precision: 1.0 Recall: 0.542 Accuracy 0.936\n",
      "No. of Estimators used: 50 / Depth: 30 ---- Precision: 1.0 Recall: 0.703 Accuracy 0.959\n",
      "No. of Estimators used: 50 / Depth: None ---- Precision: 0.992 Recall: 0.794 Accuracy 0.97\n",
      "No. of Estimators used: 100 / Depth: 10 ---- Precision: 1.0 Recall: 0.303 Accuracy 0.903\n",
      "No. of Estimators used: 100 / Depth: 20 ---- Precision: 1.0 Recall: 0.581 Accuracy 0.942\n",
      "No. of Estimators used: 100 / Depth: 30 ---- Precision: 1.0 Recall: 0.703 Accuracy 0.959\n",
      "No. of Estimators used: 100 / Depth: None ---- Precision: 1.0 Recall: 0.8 Accuracy 0.972\n"
     ]
    }
   ],
   "source": [
    "# creating nested for loop for n_est and depth defined in the function\n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Random Forest model wtih Grid Search and Cross-validation (GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "X_count_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_count.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings with GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.442637</td>\n",
       "      <td>0.724721</td>\n",
       "      <td>0.411232</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974852</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48.211681</td>\n",
       "      <td>3.069400</td>\n",
       "      <td>0.516039</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974493</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24.115877</td>\n",
       "      <td>2.300286</td>\n",
       "      <td>0.416890</td>\n",
       "      <td>0.099565</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.978437</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46.765986</td>\n",
       "      <td>0.149628</td>\n",
       "      <td>0.599653</td>\n",
       "      <td>0.079758</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.042821</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.159411</td>\n",
       "      <td>0.048053</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973594</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>5</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.998653</td>\n",
       "      <td>0.997979</td>\n",
       "      <td>0.997979</td>\n",
       "      <td>0.996857</td>\n",
       "      <td>0.997979</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10      25.442637      0.724721         0.411232        0.070535   \n",
       "11      48.211681      3.069400         0.516039        0.041333   \n",
       "7       24.115877      2.300286         0.416890        0.099565   \n",
       "8       46.765986      0.149628         0.599653        0.079758   \n",
       "6        3.042821      0.411000         0.159411        0.048053   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "10            None                150   \n",
       "11            None                300   \n",
       "7               90                150   \n",
       "8               90                300   \n",
       "6               90                 10   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.980269   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.979372   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.978475   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.979372   \n",
       "6      {'max_depth': 90, 'n_estimators': 10}           0.973991   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "10           0.976640           0.974843  ...         0.974852   \n",
       "11           0.977538           0.975741  ...         0.974493   \n",
       "7            0.978437           0.974843  ...         0.974133   \n",
       "8            0.979335           0.973944  ...         0.974133   \n",
       "6            0.977538           0.975741  ...         0.973594   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "10        0.003422                1            1.000000            1.000000   \n",
       "11        0.004169                2            1.000000            1.000000   \n",
       "7         0.004638                3            0.999326            0.999102   \n",
       "8         0.004639                3            0.999326            0.999326   \n",
       "6         0.004042                5            0.998428            0.998653   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "10            1.000000            1.000000            1.000000   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "7             0.999102            0.999551            0.998877   \n",
       "8             0.999102            0.999775            0.999102   \n",
       "6             0.997979            0.997979            0.996857   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "10          1.000000         0.000000  \n",
       "11          1.000000         0.000000  \n",
       "7           0.999192         0.000229  \n",
       "8           0.999326         0.000246  \n",
       "6           0.997979         0.000619  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our GS classifier\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_fit = gs.fit(X_tfidf_feat, data['label']) # fitting the rf model\n",
    "\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.876793</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.435337</td>\n",
       "      <td>0.052178</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973954</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.402876</td>\n",
       "      <td>0.377368</td>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973415</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.998653</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>37.849599</td>\n",
       "      <td>0.555546</td>\n",
       "      <td>0.484837</td>\n",
       "      <td>0.097426</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973235</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.302975</td>\n",
       "      <td>0.615619</td>\n",
       "      <td>0.330630</td>\n",
       "      <td>0.054201</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972517</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.089313</td>\n",
       "      <td>0.190326</td>\n",
       "      <td>0.151011</td>\n",
       "      <td>0.022885</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 10}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972157</td>\n",
       "      <td>0.003647</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994609</td>\n",
       "      <td>0.992142</td>\n",
       "      <td>0.994387</td>\n",
       "      <td>0.991917</td>\n",
       "      <td>0.991917</td>\n",
       "      <td>0.992995</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       35.876793      0.556350         0.435337        0.052178   \n",
       "7       18.402876      0.377368         0.276875        0.018294   \n",
       "11      37.849599      0.555546         0.484837        0.097426   \n",
       "10      20.302975      0.615619         0.330630        0.054201   \n",
       "3        3.089313      0.190326         0.151011        0.022885   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                300   \n",
       "7               90                150   \n",
       "11            None                300   \n",
       "10            None                150   \n",
       "3               60                 10   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.979372   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.980269   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.978475   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.977578   \n",
       "3      {'max_depth': 60, 'n_estimators': 10}           0.975785   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "8            0.975741           0.973944  ...         0.973954   \n",
       "7            0.973944           0.974843  ...         0.973415   \n",
       "11           0.975741           0.974843  ...         0.973235   \n",
       "10           0.972147           0.973046  ...         0.972517   \n",
       "3            0.971249           0.976640  ...         0.972157   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8         0.003606                1            0.999326            0.998877   \n",
       "7         0.004212                2            0.999102            0.998428   \n",
       "11        0.004045                3            1.000000            1.000000   \n",
       "10        0.003157                4            1.000000            1.000000   \n",
       "3         0.003647                5            0.994609            0.992142   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "8             0.999102            0.999326            0.998877   \n",
       "7             0.998653            0.999326            0.998877   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "3             0.994387            0.991917            0.991917   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "8           0.999102         0.000201  \n",
       "7           0.998877         0.000317  \n",
       "11          1.000000         0.000000  \n",
       "10          1.000000         0.000000  \n",
       "3           0.992995         0.001232  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our RF classifier\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_fit = gs.fit(X_count_feat, data['label']) # fitting the rf model\n",
    "\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GB) \n",
    "GB is an ensemble learning method that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations.\n",
    "\n",
    "**Pros**:\n",
    " - Extremely powerful\n",
    " - Accepts various types of inputs\n",
    " - Can be used for classification or regression\n",
    " - Outputs feature importance\n",
    "\n",
    "**Cons**:\n",
    " - Longer to train (can't be parallelized)\n",
    " - More likely to overfit\n",
    " - More difficult to properly tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Gradient Boosting Model with grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128    4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62    3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28    7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135    4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# defining parameters\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "#fitting and vectorizing count_vect to our body_text\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "\n",
    "# building a datafram\n",
    "X_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring GBClassifier Attributes and HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_SUPPORTED_LOSS', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_initialized', '_check_params', '_clear_state', '_decision_function', '_estimator_type', '_fit_stage', '_fit_stages', '_get_param_names', '_init_decision_function', '_init_state', '_is_initialized', '_make_estimator', '_resize_state', '_staged_decision_function', '_validate_estimator', '_validate_y', 'apply', 'decision_function', 'feature_importances_', 'fit', 'get_params', 'n_features', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'staged_decision_function', 'staged_predict', 'staged_predict_proba']\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(dir(GradientBoostingClassifier))\n",
    "print(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our own Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting test and train dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for RF classifier\n",
    "def train_GB(n_est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth,learning_rate=lr) # instantiate our GB classifier\n",
    "    gb_model = gb.fit(X_train, y_train) # fitting the gb model\n",
    "    y_pred = gb_model.predict(X_test) #making prediction \n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary') #generating result matrix \n",
    "    print(\"No. of Estimators used: {} / Depth: {} / Learning Rate: {} ---- Precision: {} Recall: {} Accuracy {}\".format(n_est, \n",
    "                                                                                                         depth,\n",
    "                                                                                                         lr,\n",
    "                                                                                                         round(precision, 3),\n",
    "                                                                                                         round(recall, 3),\n",
    "                                                                                                         round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saluj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.0 Recall: 0.0 Accuracy 0.866\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.973 Recall: 0.725 Accuracy 0.961\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 1 ---- Precision: 0.902 Recall: 0.745 Accuracy 0.955\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.01 ---- Precision: 1.0 Recall: 0.02 Accuracy 0.869\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.926 Recall: 0.752 Accuracy 0.959\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 1 ---- Precision: 0.907 Recall: 0.785 Accuracy 0.961\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.01 ---- Precision: 1.0 Recall: 0.013 Accuracy 0.868\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.899 Recall: 0.779 Accuracy 0.959\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 1 ---- Precision: 0.871 Recall: 0.819 Accuracy 0.96\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.01 ---- Precision: 1.0 Recall: 0.007 Accuracy 0.867\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.912 Recall: 0.765 Accuracy 0.959\n",
      "No. of Estimators used: 50 / Depth: None / Learning Rate: 1 ---- Precision: 0.844 Recall: 0.832 Accuracy 0.957\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.974 Recall: 0.51 Accuracy 0.933\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.966 Recall: 0.765 Accuracy 0.965\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 1 ---- Precision: 0.902 Recall: 0.805 Accuracy 0.962\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.959 Recall: 0.624 Accuracy 0.946\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.927 Recall: 0.765 Accuracy 0.961\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 1 ---- Precision: 0.898 Recall: 0.772 Accuracy 0.958\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.939 Recall: 0.718 Accuracy 0.956\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.901 Recall: 0.792 Accuracy 0.961\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 1 ---- Precision: 0.885 Recall: 0.826 Accuracy 0.962\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.932 Recall: 0.732 Accuracy 0.957\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.917 Recall: 0.812 Accuracy 0.965\n",
      "No. of Estimators used: 100 / Depth: None / Learning Rate: 1 ---- Precision: 0.866 Recall: 0.826 Accuracy 0.96\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.976 Recall: 0.55 Accuracy 0.938\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.966 Recall: 0.772 Accuracy 0.966\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 1 ---- Precision: 0.934 Recall: 0.758 Accuracy 0.961\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.951 Recall: 0.658 Accuracy 0.95\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.936 Recall: 0.785 Accuracy 0.964\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 1 ---- Precision: 0.896 Recall: 0.805 Accuracy 0.961\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.914 Recall: 0.711 Accuracy 0.952\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.916 Recall: 0.805 Accuracy 0.964\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 1 ---- Precision: 0.869 Recall: 0.799 Accuracy 0.957\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.01 ---- Precision: 0.933 Recall: 0.745 Accuracy 0.959\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 0.1 ---- Precision: 0.924 Recall: 0.812 Accuracy 0.966\n",
      "No. of Estimators used: 150 / Depth: None / Learning Rate: 1 ---- Precision: 0.851 Recall: 0.805 Accuracy 0.955\n"
     ]
    }
   ],
   "source": [
    "# creating nested for loop for n_est and depth defined in the function\n",
    "# it is going to take a minimum of 40 minutes to run this code\n",
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Gradient Boosting with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and cleaning raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text']) # this will fit and vectorize the data\n",
    "X_count_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(X_count.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings with GridCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>387.705381</td>\n",
       "      <td>31.857941</td>\n",
       "      <td>0.195528</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969643</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>193.099268</td>\n",
       "      <td>1.252441</td>\n",
       "      <td>0.203705</td>\n",
       "      <td>0.014330</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.978437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969463</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301.014254</td>\n",
       "      <td>2.678897</td>\n",
       "      <td>0.203527</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969463</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.180470</td>\n",
       "      <td>4.195428</td>\n",
       "      <td>0.254553</td>\n",
       "      <td>0.031022</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.962332</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968205</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285.958562</td>\n",
       "      <td>3.705573</td>\n",
       "      <td>0.189598</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968205</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5     387.705381     31.857941         0.195528        0.022183   \n",
       "1     193.099268      1.252441         0.203705        0.014330   \n",
       "3     301.014254      2.678897         0.203527        0.009649   \n",
       "0     126.180470      4.195428         0.254553        0.031022   \n",
       "4     285.958562      3.705573         0.189598        0.012141   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "5                 0.1              15                150   \n",
       "1                 0.1               7                150   \n",
       "3                 0.1              11                150   \n",
       "0                 0.1               7                100   \n",
       "4                 0.1              15                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "5  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.964126   \n",
       "1  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.965022   \n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.964126   \n",
       "0  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.962332   \n",
       "4  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.964126   \n",
       "\n",
       "   split1_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "5           0.977538  ...         0.969643        0.004437                1   \n",
       "1           0.978437  ...         0.969463        0.005103                2   \n",
       "3           0.979335  ...         0.969463        0.005164                2   \n",
       "0           0.977538  ...         0.968205        0.005304                4   \n",
       "4           0.973046  ...         0.968205        0.002913                4   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "5                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "0                 1.0                 1.0                 1.0   \n",
       "4                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "5                 1.0                 1.0               1.0              0.0  \n",
       "1                 1.0                 1.0               1.0              0.0  \n",
       "3                 1.0                 1.0               1.0              0.0  \n",
       "0                 1.0                 1.0               1.0              0.0  \n",
       "4                 1.0                 1.0               1.0              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our GS classifier\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "cv_fit = gs.fit(X_tfidf_feat, data['label']) # fitting the rf model\n",
    "\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>293.519093</td>\n",
       "      <td>1.613671</td>\n",
       "      <td>0.199143</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969822</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>377.113846</td>\n",
       "      <td>29.543739</td>\n",
       "      <td>0.197419</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969822</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277.299901</td>\n",
       "      <td>2.055962</td>\n",
       "      <td>0.205615</td>\n",
       "      <td>0.012847</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.962332</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969283</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204.725275</td>\n",
       "      <td>1.143846</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968205</td>\n",
       "      <td>0.004727</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124.647944</td>\n",
       "      <td>4.132663</td>\n",
       "      <td>0.198463</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.962332</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967667</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.99991</td>\n",
       "      <td>0.00011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3     293.519093      1.613671         0.199143        0.002615   \n",
       "5     377.113846     29.543739         0.197419        0.018796   \n",
       "4     277.299901      2.055962         0.205615        0.012847   \n",
       "2     204.725275      1.143846         0.203540        0.015488   \n",
       "0     124.647944      4.132663         0.198463        0.014606   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "3                 0.1              11                150   \n",
       "5                 0.1              15                150   \n",
       "4                 0.1              15                100   \n",
       "2                 0.1              11                100   \n",
       "0                 0.1               7                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.965022   \n",
       "5  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.964126   \n",
       "4  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.962332   \n",
       "2  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.963229   \n",
       "0  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.962332   \n",
       "\n",
       "   split1_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "3           0.976640  ...         0.969822        0.004146                1   \n",
       "5           0.975741  ...         0.969822        0.003944                1   \n",
       "4           0.977538  ...         0.969283        0.005386                3   \n",
       "2           0.975741  ...         0.968205        0.004727                4   \n",
       "0           0.979335  ...         0.967667        0.006028                5   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "3                 1.0                 1.0            1.000000   \n",
       "5                 1.0                 1.0            1.000000   \n",
       "4                 1.0                 1.0            1.000000   \n",
       "2                 1.0                 1.0            1.000000   \n",
       "0                 1.0                 1.0            0.999775   \n",
       "\n",
       "   split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "3                 1.0            1.000000           1.00000          0.00000  \n",
       "5                 1.0            1.000000           1.00000          0.00000  \n",
       "4                 1.0            1.000000           1.00000          0.00000  \n",
       "2                 1.0            1.000000           1.00000          0.00000  \n",
       "0                 1.0            0.999775           0.99991          0.00011  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our GS classifier\n",
    "# it is going to take a minimum of 40 minutes to run this code\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "cv_fit = gs.fit(X_count_feat, data['label']) # fitting the rf model\n",
    "\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection: Data Prep\n",
    "Process:\n",
    "1. Split the data into training and test set.\n",
    "2. Train vectorizers on training set and use that to transform test set.\n",
    "3. Fit best Random Forest model and best Gradient Boosting model on training set and predict on test set.\n",
    "4. Thoroughly evaluate results of these 2 models to select the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#importing Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#assigning stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# reading the text file\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# defining a function to count punctuations\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation]) # total count of punctuation\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100 # No. of punctuations divided by length of message (excluding whitespaces as we did above)\n",
    "\n",
    "# to calculate the correct length of messages, we will deduct count of whitespaces\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# to calculate % of puncuation\n",
    "data['punc%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# combining 3 seperate function to clean our text  \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting test and train dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punc%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>7093</th>\n",
       "      <th>7094</th>\n",
       "      <th>7095</th>\n",
       "      <th>7096</th>\n",
       "      <th>7097</th>\n",
       "      <th>7098</th>\n",
       "      <th>7099</th>\n",
       "      <th>7100</th>\n",
       "      <th>7101</th>\n",
       "      <th>7102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...  7093  7094  \\\n",
       "0       117    4.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        98    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        35    8.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        51   25.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4        77   13.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   7095  7096  7097  7098  7099  7100  7101  7102  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 7105 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining parameters\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# fitting tfidf_vect to our body_text\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text']) # this will only fit the data in X_train\n",
    "\n",
    "# training the fit training set\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "\n",
    "# using the same vectorizer to transform our test set\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "# tfidf_train and tfidf_test will have the same number of columns as they are both\n",
    "# transformed using tfidf_vect_fit which was trained on the training set. So it will\n",
    "# recognize only the words which are in the training set\n",
    "\n",
    "# concatenating vectorized data back with body length and punctuatuation to give us X_features\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punc%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punc%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing functions \n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: 3.451\n",
      "Preict Time: 0.226\n",
      "Precision: 1.0\n",
      "Recall: 0.764\n",
      "Accuracy 0.967\n"
     ]
    }
   ],
   "source": [
    "# instantiate our RF classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1) # n_jobs=-1 allows model to run faster by building the individual decision trees in parallel\n",
    "\n",
    "start = time.time()\n",
    "# fitting the rf model\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "fit_time = (end - start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# making prediction\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "pred_time = (end - start)\n",
    "\n",
    "# looking at actual performance metrics\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary') \n",
    "\n",
    "print(\"Fit Time: {}\\nPreict Time: {}\\nPrecision: {}\\nRecall: {}\\nAccuracy {}\".format(\n",
    "                                                         round(fit_time, 3),\n",
    "                                                         round(pred_time, 3),\n",
    "                                                         round(precision, 3),\n",
    "                                                         round(recall, 3),\n",
    "                                                         round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing results with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: 344.131\n",
      "Preict Time: 0.151\n",
      "Precision: 0.939\n",
      "Recall: 0.783\n",
      "Accuracy 0.962\n"
     ]
    }
   ],
   "source": [
    "# instantiate our GB classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "# fitting the gb model\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "fit_time = (end - start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# making prediction\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "pred_time = (end - start)\n",
    "\n",
    "# looking at actual performance metrics\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary') \n",
    "\n",
    "print(\"Fit Time: {}\\nPreict Time: {}\\nPrecision: {}\\nRecall: {}\\nAccuracy {}\".format(\n",
    "                                                         round(fit_time, 3),\n",
    "                                                         round(pred_time, 3),\n",
    "                                                         round(precision, 3),\n",
    "                                                         round(recall, 3),\n",
    "                                                         round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Amandeep Saluja"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
